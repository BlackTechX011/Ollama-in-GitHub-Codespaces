{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Run Ollama in Google Colab",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text"
      },
      "source": [
        "# Run Ollama in Google Colab\n",
        "Learn how to set up and run Ollama in Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text"
      },
      "source": [
        "## What is Ollama?\n",
        "Ollama is an open-source project that provides a powerful and user-friendly platform for running Large Language Models (LLMs) on your local machine. It simplifies the process of using advanced AI models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text"
      },
      "source": [
        "## Setting Up Ollama in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "download-and-install-ollama"
      },
      "source": [
        "# Download and install Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "verify-installation"
      },
      "source": [
        "# Verify the installation\n",
        "!ollama"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ollama CLI version x.x.x\n",
            "Usage: ollama [command] [options]\n",
            "... (more commands) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "start-ollama"
      },
      "source": [
        "# Start Ollama\n",
        "!ollama serve"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Ollama server...\n",
            "Server running on http://localhost:8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "run-llama3"
      },
      "source": [
        "# Run and chat with Llama 3\n",
        "!ollama run llama3"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Llama 3 model...\n",
            "Welcome to Llama 3. How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text"
      },
      "source": [
        "## Available Models\n",
        "Ollama supports a variety of models that you can use. Below are some example models with their respective commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text"
      },
      "source": [
        "| Model              | Parameters | Size  | Command                       |\n",
        "| ------------------ | ---------- | ----- | ----------------------------- |\n",
        "| Llama 3            | 8B         | 4.7GB | `!ollama run llama3`           |\n",
        "| Llama 3            | 70B        | 40GB  | `!ollama run llama3:70b`       |\n",
        "| Phi 3 Mini         | 3.8B       | 2.3GB | `!ollama run phi3`             |\n",
        "| Phi 3 Medium       | 14B        | 7.9GB | `!ollama run phi3:medium`      |\n",
        "| Gemma              | 2B         | 1.4GB | `!ollama run gemma:2b`         |\n",
        "| Gemma              | 7B         | 4.8GB | `!ollama run gemma:7b`         |\n",
        "| Mistral            | 7B         | 4.1GB | `!ollama run mistral`          |\n",
        "| Moondream 2        | 1.4B       | 829MB | `!ollama run moondream`        |\n",
        "| Neural Chat        | 7B         | 4.1GB | `!ollama run neural-chat`      |\n",
        "| Starling           | 7B         | 4.1GB | `!ollama run starling-lm`      |\n",
        "| Code Llama         | 7B         | 3.8GB | `!ollama run codellama`        |\n",
        "| Llama 2 Uncensored | 7B         | 3.8GB | `!ollama run llama2-uncensored`|\n",
        "| LLaVA              | 7B         | 4.5GB | `!ollama run llava`            |\n",
        "| Solar              | 10.7B      | 6.1GB | `!ollama run solar`            |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text"
      },
      "source": [
        "> **Important:**\n",
        "> Ensure that your system meets the following RAM requirements:\n",
        "> - At least 8 GB of RAM for 7B models\n",
        "> - At least 16 GB of RAM for 13B models\n",
        "> - At least 32 GB of RAM for 33B models"
      ]
    }
  ]
}
